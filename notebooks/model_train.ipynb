{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-24T06:45:59.459561900Z",
     "start_time": "2023-07-24T06:45:59.086695100Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def merge_csv_files(input_dir, group_size=10):\n",
    "    csv_files = [file for file in os.listdir(input_dir) if file.endswith(\".csv\")]\n",
    "\n",
    "    grouped_files = [\n",
    "        csv_files[i : i + group_size] for i in range(0, len(csv_files), group_size)\n",
    "    ]\n",
    "\n",
    "    data = []\n",
    "    label = []\n",
    "\n",
    "    for group_idx, group in enumerate(grouped_files):\n",
    "        group_data = []\n",
    "\n",
    "        for csv_file in group:\n",
    "            file_path = os.path.join(input_dir, csv_file)\n",
    "            df = pd.read_csv(file_path, header=None)\n",
    "            pure_data = df.iloc[1:, 1:-1]\n",
    "            group_data.append(pure_data.to_numpy(dtype=np.float64).flatten())\n",
    "\n",
    "        group_data = np.array(group_data, dtype=np.float64)\n",
    "        group_label = int(df.iloc[1, -1])\n",
    "        label.append(group_label)\n",
    "        data.append(group_data)\n",
    "\n",
    "    return np.array(data), np.array(label)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T06:45:59.475600700Z",
     "start_time": "2023-07-24T06:45:59.463561500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "input_directory = \"../data/extract_data/train\"\n",
    "data_set = merge_csv_files(input_directory)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T06:46:01.489807300Z",
     "start_time": "2023-07-24T06:45:59.477608800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class CostumDataset(Dataset):\n",
    "    def __init__(self, file_path, transform=None):\n",
    "        self.data, self.labels = merge_csv_files(file_path)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample, label\n",
    "\n",
    "train_data = CostumDataset(\"../data/extract_data/train\")\n",
    "val_data = CostumDataset(\"../data/extract_data/val\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T06:46:05.091630Z",
     "start_time": "2023-07-24T06:46:01.490799100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据集的长度为:171\n",
      "验证数据集的长度为:57\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "os.environ['TORCH_HOME'] = r'C:\\Users\\bugs_\\PycharmProjects\\eegProject\\models'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_data_size = len(train_data)\n",
    "val_data_size = len(val_data)\n",
    "print(\"训练数据集的长度为:{}\".format(train_data_size))\n",
    "print(\"验证数据集的长度为:{}\".format(val_data_size))\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=1)\n",
    "val_dataloader = DataLoader(val_data, batch_size=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T06:48:46.556620Z",
     "start_time": "2023-07-24T06:48:46.547989700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building model..\n",
      "GRU(\n",
      "  (gru_layer): GRU(176, 64, num_layers=2, batch_first=True)\n",
      "  (out): Linear(in_features=64, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "print('==> Building model..')\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GRU, self).__init__()\n",
    "\n",
    "        self.gru_layer = nn.GRU(\n",
    "            input_size=176,\n",
    "            hidden_size=64,\n",
    "            num_layers=2,\n",
    "            bias=True,\n",
    "            batch_first=True,       # input & output will has batch size as 1s dimension. e.g. (batch, segment_length, no_feature)\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(64, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r_out, (h_n, h_c) = self.gru_layer(x.float(), None)\n",
    "        r_out = F.dropout(r_out, 0.3)\n",
    "        test_output = self.out(r_out[:, -1, :]) # choose r_out at the last time step\n",
    "        return test_output\n",
    "\n",
    "gru = GRU()\n",
    "gru.to(device)\n",
    "\n",
    "print(gru)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T07:15:01.411544300Z",
     "start_time": "2023-07-24T07:15:01.397481300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn.to(device)\n",
    "\n",
    "learning_rate = 1e-2\n",
    "optimizer = torch.optim.Adam(gru.parameters(), lr=learning_rate, weight_decay=0.005)   # optimize all parameters\n",
    "\n",
    "\n",
    "total_train_step = 0\n",
    "total_val_step = 0\n",
    "epoch = 100"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T07:44:03.496791500Z",
     "start_time": "2023-07-24T07:44:03.482754600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 176])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for data in train_dataloader:\n",
    "    features, targets = data\n",
    "    for i in features[0]:\n",
    "        i = torch.unsqueeze(i, 0)\n",
    "print(i.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T08:08:26.296485100Z",
     "start_time": "2023-07-24T08:08:26.268485700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------第1轮训练开始--------\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[38], line 16\u001B[0m\n\u001B[0;32m     13\u001B[0m     i \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39munsqueeze(i, \u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m     14\u001B[0m     i \u001B[38;5;241m=\u001B[39m i\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m---> 16\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mgru\u001B[49m\u001B[43m(\u001B[49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     17\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_fn(outputs, targets\u001B[38;5;241m.\u001B[39mlong())\n\u001B[0;32m     19\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\eegEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[1;32mIn[14], line 21\u001B[0m, in \u001B[0;36mGRU.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     19\u001B[0m r_out, (h_n, h_c) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgru_layer(x\u001B[38;5;241m.\u001B[39mfloat(), \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m     20\u001B[0m r_out \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mdropout(r_out, \u001B[38;5;241m0.3\u001B[39m)\n\u001B[1;32m---> 21\u001B[0m test_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mout(\u001B[43mr_out\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m) \u001B[38;5;66;03m# choose r_out at the last time step\u001B[39;00m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m test_output\n",
      "\u001B[1;31mIndexError\u001B[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "writer = SummaryWriter(r\"C:\\Users\\bugs_\\PycharmProjects\\eegProject\\loss_train\")\n",
    "\n",
    "for i in range(epoch):\n",
    "    print(\"--------第{}轮训练开始--------\".format(i + 1))\n",
    "\n",
    "    gru.train()\n",
    "    for data in train_dataloader:\n",
    "        features, targets = data\n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "        for i in features[0]:\n",
    "            i = torch.unsqueeze(i, 0)\n",
    "\n",
    "            outputs = gru(i)\n",
    "        loss = loss_fn(outputs, targets.long())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_step += 1\n",
    "        # if total_train_step % 100 == 0:\n",
    "        print(\"训练次数:{}, Loss:{}\".format(total_train_step, loss.item()))\n",
    "        end_time = time.time()\n",
    "        print(\"耗时：{}s\".format(end_time - start_time))\n",
    "        writer.add_scalar(\"train_loss\", loss.item(), total_train_step)\n",
    "\n",
    "    gru.eval()\n",
    "    total_val_loss = 0\n",
    "    total_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_dataloader:\n",
    "            features, targets = data\n",
    "            features = features.to(device)\n",
    "            targets = targets.to(device)\n",
    "            for i in features[0]:\n",
    "                i = torch.unsqueeze(i, 0)\n",
    "                outputs = gru(i)\n",
    "            outputs = gru(features)\n",
    "            loss = loss_fn(outputs, targets.long())\n",
    "            total_val_loss = total_val_loss + loss.item()\n",
    "            accuracy = (outputs.argmax(1) == targets).sum()\n",
    "            total_accuracy = total_accuracy + accuracy\n",
    "    print(\"整体验证集上的Loss：{}\".format(total_val_loss))\n",
    "    print(\"整体验证集上的正确率：{}\".format(total_accuracy / val_data_size))\n",
    "    writer.add_scalar(\"val_loss\", total_val_loss, total_val_step)\n",
    "    writer.add_scalar(\"val_accuracy\", total_accuracy / val_data_size, total_val_step)\n",
    "    total_val_step += 1\n",
    "\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T08:10:18.055830500Z",
     "start_time": "2023-07-24T08:10:17.993782300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
