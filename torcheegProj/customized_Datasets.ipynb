{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 设置文件夹路径和类别名称\n",
    "folders = [r'..\\data\\Test_EEG\\HC',\n",
    "           r'..\\data\\Test_EEG\\MDD',\n",
    "           r'..\\data\\Test_EEG\\BD']\n",
    "class_names = ['HC', 'MDD', 'BD']\n",
    "\n",
    "# 创建存储数据集的字典\n",
    "data_file = {'filename': []}\n",
    "\n",
    "# 遍历每个类别的文件夹\n",
    "for folder, class_name in zip(folders, class_names):\n",
    "    # 获取文件夹中的Clean.mat文件列表\n",
    "    file_list = os.listdir(folder)\n",
    "    file_list = [os.path.join(folder, file) for file in file_list if file.endswith('Clean.mat')]\n",
    "\n",
    "    # 将数据加入到对应的数据集中\n",
    "    data_file['filename'].extend([(file, class_name) for file in file_list])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T08:51:47.153207700Z",
     "start_time": "2023-07-25T08:51:47.140201200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "\n",
    "label_dic = {'HC': 0, 'MDD': 1, \"BD\": 2}\n",
    "target_shape = (284, 16, 24000)\n",
    "X = np.empty(target_shape)\n",
    "y = {'label': np.empty(284, dtype=int)}\n",
    "\n",
    "num_merged = 0\n",
    "\n",
    "for sub, label in data_file['filename']:  # 被试循环\n",
    "\n",
    "    data = sio.loadmat(sub)\n",
    "    sample = data['EEG_ECClean']\n",
    "    eeg_data = sample[\"data\"][0][0][:, 0:24000]\n",
    "    X[num_merged] = eeg_data\n",
    "    y['label'][num_merged] = label_dic[label]\n",
    "    num_merged += 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T08:51:51.448047Z",
     "start_time": "2023-07-25T08:51:47.146187500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NUMPY]:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current io_size is not enough, and double the LMDB map size to 20971520 automatically.\n",
      "The current io_size is not enough, and double the LMDB map size to 41943040 automatically.\n",
      "The current io_size is not enough, and double the LMDB map size to 83886080 automatically.\n",
      "The current io_size is not enough, and double the LMDB map size to 167772160 automatically.\n",
      "The current io_size is not enough, and double the LMDB map size to 335544320 automatically.\n",
      "The current io_size is not enough, and double the LMDB map size to 671088640 automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NUMPY]:  50%|█████     | 1/2 [00:01<00:01,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current io_size is not enough, and double the LMDB map size to 1342177280 automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NUMPY]: 100%|██████████| 2/2 [00:02<00:00,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait for the writing process to complete...\n",
      "(array([[-1.44045854, -0.43012276,  1.58365417, ..., -5.12197399,\n",
      "        -4.41983366, -3.1613338 ],\n",
      "       [-2.54928565, -3.83311224, -3.79554558, ..., -3.7846837 ,\n",
      "        -3.47788382, -3.46887398],\n",
      "       [-0.90644282, -0.02179096,  0.66771239, ..., -0.72422498,\n",
      "        -1.52700377, -1.8246603 ],\n",
      "       ...,\n",
      "       [-0.60493821, -0.05604951,  0.75064021, ...,  1.48914242,\n",
      "         1.95111287,  1.68770337],\n",
      "       [ 0.40879893, -1.10817182, -1.86888313, ..., -0.81465912,\n",
      "        -1.5469166 , -1.16325331],\n",
      "       [ 2.43667531,  3.24103284,  2.26753902, ...,  5.27654648,\n",
      "         5.41562891,  4.28534937]]), {'clip_id': '0_0', 'label': 0})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torcheeg import transforms\n",
    "from torcheeg.datasets import NumpyDataset\n",
    "\n",
    "dataset = NumpyDataset(X=X, y=y)\n",
    "print(dataset[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T08:51:57.269637500Z",
     "start_time": "2023-07-25T08:51:51.448047Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from torcheeg.model_selection import KFold\n",
    "\n",
    "k_fold = KFold(n_splits=10,\n",
    "                      shuffle=True,\n",
    "                      random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T08:42:03.601883400Z",
     "start_time": "2023-07-25T08:42:03.578955300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels=16, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, 64, kernel_size=4, stride=1, padding=2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(64, 128, kernel_size=4, stride=1, padding=2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(128, 256, kernel_size=4, stride=1, padding=2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv1d(256, 64, kernel_size=4, stride=1, padding=2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Calculate the output size of the convolutional layers\n",
    "        self.output_size = self._get_conv_output_size()\n",
    "\n",
    "        self.lin1 = nn.Linear(self.output_size, 1024)\n",
    "        self.lin2 = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def _get_conv_output_size(self):\n",
    "        # Dummy input to calculate the output size of the convolutional layers\n",
    "        dummy_input = torch.zeros(1, 16, 24000)\n",
    "        x = self._forward_conv(dummy_input)\n",
    "        return x.view(x.size(0), -1).size(1)\n",
    "\n",
    "    def _forward_conv(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._forward_conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.lin1(x)\n",
    "        x = self.lin2(x)\n",
    "        return x\n",
    "# class CNN(torch.nn.Module):\n",
    "#     def __init__(self, in_channels=4, num_classes=3):\n",
    "#         super().__init__()\n",
    "#         self.conv1 = nn.Sequential(\n",
    "#             nn.ZeroPad2d((1, 2, 1, 2)),\n",
    "#             nn.Conv2d(in_channels, 64, kernel_size=4, stride=1),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "#         self.conv2 = nn.Sequential(\n",
    "#             nn.ZeroPad2d((1, 2, 1, 2)),\n",
    "#             nn.Conv2d(64, 128, kernel_size=4, stride=1),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "#         self.conv3 = nn.Sequential(\n",
    "#             nn.ZeroPad2d((1, 2, 1, 2)),\n",
    "#             nn.Conv2d(128, 256, kernel_size=4, stride=1),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "#         self.conv4 = nn.Sequential(\n",
    "#             nn.ZeroPad2d((1, 2, 1, 2)),\n",
    "#             nn.Conv2d(256, 64, kernel_size=4, stride=1),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "#\n",
    "#         self.lin1 = nn.Linear(9 * 9 * 64, 1024)\n",
    "#         self.lin2 = nn.Linear(1024, num_classes)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.conv3(x)\n",
    "#         x = self.conv4(x)\n",
    "#\n",
    "#         x = x.flatten(start_dim=1)\n",
    "#         x = self.lin1(x)\n",
    "#         x = self.lin2(x)\n",
    "#         return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T08:42:04.431503800Z",
     "start_time": "2023-07-25T08:42:04.413505200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "batch_size = 32"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T08:42:05.562189500Z",
     "start_time": "2023-07-25T08:42:05.527647400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "#\n",
    "# for i, (train_dataset, val_dataset) in enumerate(k_fold.split(dataset)):\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "#     for batch_idx, batch in enumerate(train_loader):\n",
    "#         X = batch[0]\n",
    "#         y = batch[1]['label']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T08:42:06.361940200Z",
     "start_time": "2023-07-25T08:42:06.352929300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# X.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T08:42:06.697426400Z",
     "start_time": "2023-07-25T08:42:06.691423600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        X = batch[0].to(device)\n",
    "        y = batch[1]['label'].to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            loss, current = loss.item(), batch_idx * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def valid(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    val_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            X = batch[0].to(device)\n",
    "            y = batch[1].to(device)\n",
    "\n",
    "            pred = model(X)\n",
    "            val_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    val_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {val_loss:>8f} \\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T08:42:10.817523400Z",
     "start_time": "2023-07-25T08:42:10.795283200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 5.86 GiB (GPU 0; 4.00 GiB total capacity; 914.50 KiB already allocated; 3.30 GiB free; 2.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 5\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DataLoader\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, (train_dataset, val_dataset) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(k_fold\u001B[38;5;241m.\u001B[39msplit(dataset)):\n\u001B[1;32m----> 5\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[43mCNN\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m     optimizer \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mAdam(model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-4\u001B[39m)\n\u001B[0;32m      8\u001B[0m     train_loader \u001B[38;5;241m=\u001B[39m DataLoader(train_dataset, batch_size\u001B[38;5;241m=\u001B[39mbatch_size, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\eegEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1145\u001B[0m, in \u001B[0;36mModule.to\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1141\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1142\u001B[0m                     non_blocking, memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format)\n\u001B[0;32m   1143\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, non_blocking)\n\u001B[1;32m-> 1145\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\eegEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn)\u001B[0m\n\u001B[0;32m    795\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_apply\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn):\n\u001B[0;32m    796\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[1;32m--> 797\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    799\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[0;32m    800\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[0;32m    801\u001B[0m             \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[0;32m    802\u001B[0m             \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    807\u001B[0m             \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[0;32m    808\u001B[0m             \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\eegEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py:820\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn)\u001B[0m\n\u001B[0;32m    816\u001B[0m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[0;32m    817\u001B[0m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[0;32m    818\u001B[0m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[0;32m    819\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m--> 820\u001B[0m     param_applied \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    821\u001B[0m should_use_set_data \u001B[38;5;241m=\u001B[39m compute_should_use_set_data(param, param_applied)\n\u001B[0;32m    822\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m should_use_set_data:\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\eegEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1143\u001B[0m, in \u001B[0;36mModule.to.<locals>.convert\u001B[1;34m(t)\u001B[0m\n\u001B[0;32m   1140\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m convert_to_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m t\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m):\n\u001B[0;32m   1141\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1142\u001B[0m                 non_blocking, memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format)\n\u001B[1;32m-> 1143\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_floating_point\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_complex\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnon_blocking\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 5.86 GiB (GPU 0; 4.00 GiB total capacity; 914.50 KiB already allocated; 3.30 GiB free; 2.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "for i, (train_dataset, val_dataset) in enumerate(k_fold.split(dataset)):\n",
    "\n",
    "    model = CNN().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    epochs = 50\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train(train_loader, model, loss_fn, optimizer)\n",
    "        valid(val_loader, model, loss_fn)\n",
    "    print(\"Done!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T08:42:23.062586Z",
     "start_time": "2023-07-25T08:42:16.121021500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
